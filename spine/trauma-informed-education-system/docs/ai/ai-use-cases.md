# AI Use Cases

## Purpose

This document defines approved use cases for AI in the trauma-informed education system. It explicitly lists what AI can and cannot do, and defines context limits for AI inputs.

---

## Approved Use Cases

### Use Case 1: Staff Reflection Prompts (Non-Disclosive)

**Purpose:** Provide reflection prompts that support learning without requiring personal disclosure.

**What AI can do:**
- Generate reflection prompts focused on practice
- Provide frameworks for reflection
- Support thinking about principles and application
- Help structure reflection without requiring disclosure

**What AI cannot do:**
- Require personal disclosure or sharing
- Provide therapeutic reflection prompts
- Analyse personal experiences
- Create dependency on AI for reflection

**Context limits:**
- Inputs: Practice scenarios, principles, frameworks
- Outputs: Reflection prompts, questions, frameworks
- No personal information required
- No emotional disclosure required

**Example:**
- Input: "I want to reflect on how predictability applies to my classroom"
- Output: Reflection prompts focused on classroom practice, not personal experience

---

### Use Case 2: Leadership Policy Wording Support

**Purpose:** Help leaders draft policy language that's safe, clear, and inspection-ready.

**What AI can do:**
- Help shape policy language using trauma-informed principles
- Provide inspection-safe wording
- Support clear, non-clinical language
- Help integrate trauma-informed principles into policy

**What AI cannot do:**
- Write policy without leader input
- Make clinical or therapeutic claims
- Promise specific outcomes
- Override professional judgment

**Context limits:**
- Inputs: Policy drafts, principles, inspection requirements
- Outputs: Language suggestions, wording options, framing
- No clinical content
- No outcome promises

**Example:**
- Input: "How can I describe our trauma-informed approach in our behaviour policy?"
- Output: Policy-safe language that describes approach without clinical claims

---

### Use Case 3: Parent Communication Drafting (Neutral Tone)

**Purpose:** Help staff draft parent communications that are clear, respectful, and boundary-appropriate.

**What AI can do:**
- Help shape communication language
- Provide neutral, non-clinical wording
- Support clear, respectful communication
- Help maintain professional boundaries in communication

**What AI cannot do:**
- Write communications without staff input
- Provide clinical interpretations
- Make promises about outcomes
- Override safeguarding procedures

**Context limits:**
- Inputs: Communication drafts, principles, boundaries
- Outputs: Language suggestions, wording options, framing
- No clinical content
- No outcome promises
- No personal information about children

**Example:**
- Input: "How can I communicate with parents about our trauma-informed approach?"
- Output: Neutral, clear language that explains approach without clinical claims

---

### Use Case 4: Inspection-Safe Language Checks

**Purpose:** Review language for inspection safety and appropriateness.

**What AI can do:**
- Check language for clinical terms
- Identify pathologising language
- Suggest safer alternatives
- Ensure language is inspection-ready

**What AI cannot do:**
- Guarantee inspection outcomes
- Make claims about inspection results
- Override professional judgment
- Add clinical content

**Context limits:**
- Inputs: Text, language, documents
- Outputs: Language checks, suggestions, alternatives
- No clinical content
- No outcome guarantees

**Example:**
- Input: "Is this language safe for inspection?"
- Output: Language check with suggestions for safer alternatives

---

## Explicit Non-Use Cases List

### AI Cannot Provide Therapy

**Prohibition:** AI must never provide therapy, therapeutic techniques, or therapeutic interventions.

**Why:** Therapy requires professional training and appropriate boundaries. AI cannot provide therapy.

**What to do instead:** Redirect to appropriate professional support.

---

### AI Cannot Diagnose or Assess

**Prohibition:** AI must never diagnose, assess, or label children, adults, or situations.

**Why:** Diagnosis requires professional training and assessment. AI cannot provide diagnosis.

**What to do instead:** Redirect to appropriate professionals for assessment.

---

### AI Cannot Provide Child Advice

**Prohibition:** AI must never provide specific advice about individual children.

**Why:** Advice about individual children requires professional assessment and context. AI cannot provide this.

**What to do instead:** Redirect to principles and processes. Remind about safeguarding procedures.

---

### AI Cannot Predict Outcomes

**Prohibition:** AI must never predict outcomes, behaviours, or futures.

**Why:** Predictions create false expectations and cannot be guaranteed. Outcomes vary based on many factors.

**What to do instead:** Focus on creating conditions and learning, not predicting outcomes.

---

### AI Cannot Override Safeguarding

**Prohibition:** AI must never override, replace, or bypass safeguarding procedures.

**Why:** Safeguarding procedures are non-negotiable. AI must always defer to safeguarding.

**What to do instead:** Insist on following safeguarding procedures. Do not provide advice that might bypass safeguarding.

---

### AI Cannot Create Dependency

**Prohibition:** AI must never create dependency on the system, founder, or AI itself.

**Why:** The system must work without founder presence. Dependency undermines sustainability.

**What to do instead:** Support independent use. Provide frameworks, not dependencies.

---

## Context Limits: What Inputs AI Can Accept

### Allowed Inputs

**Practice scenarios:** Scenarios about practice, not personal experiences.

**Principles and frameworks:** Trauma-informed principles and frameworks.

**Language and wording:** Text, language, or documents for review or improvement.

**Policy and documentation:** Policy drafts, documentation, or communication drafts.

**Reflection frameworks:** Frameworks for reflection, not personal reflection content.

---

### Disallowed Inputs

**Personal information:** Personal experiences, trauma history, or emotional disclosures.

**Child-specific information:** Detailed information about individual children beyond what's necessary for practice scenarios.

**Clinical content:** Clinical assessments, diagnoses, or therapeutic content.

**Safeguarding details:** Detailed safeguarding information beyond what's necessary for process support.

**Emotional content:** Emotional disclosures or personal sharing.

---

### Input Boundaries

**No personal disclosure:** AI cannot accept or process personal disclosure or sharing.

**No child analysis:** AI cannot accept detailed information about individual children for analysis.

**No clinical content:** AI cannot accept clinical assessments, diagnoses, or therapeutic content.

**No safeguarding details:** AI cannot accept detailed safeguarding information beyond process support.

**No emotional content:** AI cannot accept emotional disclosures or personal sharing.

---

## Use Case Quality Checklist

Before using AI for any purpose, check:

- [ ] Is this an approved use case?
- [ ] Are inputs within allowed limits?
- [ ] Are outputs non-clinical and non-therapeutic?
- [ ] Are boundaries maintained?
- [ ] Is safeguarding protected?
- [ ] Is founder dependency prevented?

If any answer is "no," do not use AI for this purpose.

---

## Version

This is Version 1. AI use cases may evolve based on learning and feedback, but any changes will be documented and explained.
